---
title: Structured Logging
authors:
  - "@serathius"
  - "@44past4"
  - "@DirectXMan12"
owning-sig: sig-instrumentation
participating-sigs:
  - sig-architecture
reviewers:
  - "@thockin"
  - "@bgrant0507"
  - "@dims"
approvers:
  - "@brancz"
  - "@piosz"
editor: TBD
creation-date: 2019-11-15
last-updated: 2020-01-30
status: provisional
see-also:
replaces:
superseded-by:
---

# Structured Logging

<!-- toc -->
- [Summary](#summary)
- [Motivation](#motivation)
  - [Goals](#goals)
  - [Non-Goals](#non-goals)
- [Proposal](#proposal)
  - [Log message structure](#log-message-structure)
    - [New klog methods](#new-klog-methods)
  - [References to Kubernetes objects](#references-to-kubernetes-objects)
  - [Selecting most important logs](#selecting-most-important-logs)
    - [JSON output format](#json-output-format)
  - [Logging configuration](#logging-configuration)
  - [Migration / Graduation Criteria](#migration--graduation-criteria)
    - [Alpha](#alpha)
    - [Beta](#beta)
    - [GA](#ga)
  - [Performance](#performance)
    - [Logger implementation performance](#logger-implementation-performance)
    - [Log volume increase analysis](#log-volume-increase-analysis)
  - [Future work](#future-work)
  - [Risks and Mitigations](#risks-and-mitigations)
    - [Migration being abandoned halfway](#migration-being-abandoned-halfway)
    - [Huge increase of log volume](#huge-increase-of-log-volume)
- [Design Details](#design-details)
  - [Example of migrating klog call](#example-of-migrating-klog-call)
- [Alternatives](#alternatives)
    - [Just write guideline and update log messages](#just-write-guideline-and-update-log-messages)
    - [Replace klog with some other structured logging library](#replace-klog-with-some-other-structured-logging-library)
    - [Use glogr instead of proposed message structure](#use-glogr-instead-of-proposed-message-structure)
- [Code organisation / infrastructure needed](#code-organisation--infrastructure-needed)
- [Production Readiness Review Process](#production-readiness-review-process)
<!-- /toc -->

## Summary

This KEP proposes to define standard structure for Kubernetes log messages, add methods to klog to enforce this structure, add ability to configure Kubernetes components to produce logs in JSON format and initiate migration to structured logging.

## Motivation

Current logging in the Kubernetes control plane doesn’t guarantee any uniform structure for log messages and references to Kubernetes objects in those logs. This makes parsing, processing, storing, querying and analyzing logs hard and forces administrators and developers to rely on ad-hoc solutions in most cases based on some regular expressions. Due to those problems any analytical solution based on those logs is hard to implement and maintain.

### Goals

* Improve querability of most common logs in Kubernetes by standardizing log message and references to Kubernetes objects (Pods, Nodes etc.)
* Enforce log structure by introduction of new klog methods that could be used to generate structured logs.
* Propose reasonable scope of migration:
  * Executable within SIG-instrumentation resources
  * Minimize review burden on other SIGs
  * Leave logging in acceptable state if effort fails
* Simplify ingestion of logs into third party logging solutions by adding an option to output logs in the JSON format


### Non-Goals

* We are *not* replacing currently used logging library (klog) or the way in which it is used
* We are *not* preserving full compatibility of logging output
* We are *not* proposing structuring all logs in Kubernetes

## Proposal

This KEP proposes introducing a standardized log message structure. To enforce a new log message structure we are proposing adding new methods to the klog library: `InfoS`, `WarningS` , `ErrorS`  and `FatalS` which will provide a structured interface for building log messages and creating new helper methods which will provide consistent identification of Kubernetes objects in logs.

With the new methods we would like also to migrate to klog v2 and introduce a new JSON logging output format to Kubernetes components that would be an alternative to the current text format and which will make querying and processing Kubernetes logs even simpler.

Taking into account the size of Kubernetes repository we will not tackle the problem of full migration to new klog methods but instead focus on improving the querability of most common logs. We will be targeting reaching a high percentage of logs in a new format generated by real cluster.

### Log message structure

We would like for the Kubernetes community to settle on one preferred log message structure. Proposed structure should:
* Separate log message from its arguments
* Treat log arguments as key-value pairs
* Be easily parsable and queryable
* Have a specific guidance on log message and its arguments

For this purpose we suggest to use following log message structure:
```
<message> <key1>=<value1> <key2>=<value2> ...
```

where
* message is formatted using “%q” fmt logic
* keys are formatted using “%s” fmt logic
* values are formatted using “%v” fmt logic and optionally escaped using the “%q” fmt logic if it not a number or a boolean
* keys are separated from values with a “=” sign
* message and key-values are separated with a single space

#### New klog methods

To enforce a new log message structure we want to introduce new methods to klog library which will provide a more structured interface for formatting log messages compared to current methods based on fmt format strings.
For each format method (`Infof`, `Warningf`, `Errorf`, `Fatalf`) we will add matching structured method (`InfoS`, `WarningS`, `ErrorS`, `FatalS`).
Each of those methods will accept log messages as a first argument and a list of key-values pairs as a variadic second argument.
Example:

```go
klog.InfoS("Pod status updated", "pod", "kubedns", "status", "ready")
```

That would result in log
```
I1025 00:15:15.525108       1 controller_utils.go:116] “Pod status updated“ pod=“kubedns“ status=“ready“
```

### References to Kubernetes objects

Currently there is no consistency when logging references to Kubenetes objects like Pods. Depending on a log just the name of a Pod, the name and namespace as separate fields or Pod name prefixed with a namespace and ‘/’ character is logged.
This makes finding logs related to a given object much harder.
To address this problem we want to implement helper functions for all standard object types which could be used to build values which when logged will provide a consistent identification of each of those objects.
For instance for namespaced objects those methods should produce following value: “<namespace>/<name>”.

Example:
```go
klog.InfoS("Pod status updated", "pod", metadata.Pod(pod), "status", "ready")
```
That would result in log
```
I1025 00:15:15.525108       1 controller_utils.go:116] “Pod status updated“ pod=“kube-system/kubedns“ status=“ready“
```

### Selecting most important logs

As migration to new message structure will be done manually we will be focusing on logs that have the greatest impact for log querying and processing. We will be focusing on log messages Proposed plan of measurements:

* Using log data from single run of kubernetes [gce-master-scale-performance](https://k8s-testgrid.appspot.com/sig-scalability-gce#gce-master-scale-performance) tests
* Based on logs generated on master node
* Aggregate by components:
  * kube-controller-manager
  * kube-scheduler
  * kube-apiserver
  * kubelet
* Only for log lines generated by klog (e.g. prefix `I0129 03:26:19.857798       1 garbagecollector.go:517]`)
* Identified by code line that generated log (e.g. `garbagecollector.go:517`)

We assume that exact details of taking measurements can be improved, but overall methodology is solid. With those criteria we concluded that covering 99.9% of logs on master node are generated by 22 log messages. Exact list of those logs is provided in the detail design section.

#### JSON output format

Introduction of new methods to klog library will make identification of different components of the log message much easier.
With klog v2 we can take further advantage of this fact and add an option to produce structured logs in JSON format.

Some pros of using JSON:
* Broadly adopted by logging libraries with very efficient implementations (zap, zerolog).
* Out of the box support by many logging backends (Elasticsearch, Stackdriver, BigQuery, Splunk)
* Easily parsable and transformable
* Existing tools for ad-hoc analysis (jq)

Example:

```go
klog.Infof("Updated pod %s status to ready", pod.name)
```
That would result in log

```json
{
   "ts": 1580306777.04728,
   "level": "info",
   "msg": "Updated pod kubedns status to ready"
}
```

And

```go
klog.InfoS("Pod status updated", "pod"", metadata.Pod(pod), "status"", "ready")
```

That would result in log
```json
{
   "ts": 1580306777.04728,
   "level": "info",
   "msg": "Pod status updated",
   "pod":{
      "name": "nginx-1",
      "namespace": "default"
   },
   "status": "ready"
}
```


### Logging configuration

To allow selection of the logging output format we would like to introduce a new logging configuration shared by all kubernetes components.

`LoggingConfig` structure should be implemented as part of `k8s.io/component-base` options and include a common set of flags for logger initialization.

`--logging-format` flag should allow you to pick between logging logging output formats. Setting this flag will select a particular logger implementation.

Proposed flag `--logging-format` values:
* `text` for old text-based logging format
* `json` for new JSON format

### Migration / Graduation Criteria

#### Alpha

Preparation and migration of selected logs to new logging methods:
* Support for the new InfoS, WarningS, ErrorS and FatalS method will be implemented in klog and the new version of klog will be released.
* Kubernetes will be upgraded to use the new klog version.
* Helper methods for producing consistent Kubernetes resource identifiers to be used in logs will be implemented.
* Most important klog calls will be identified and will be manually migrated from using format based to structured methods.

#### Beta

Adding support for JSON output format and guarding against regression:
* klog version used by Kubernetes and all of its dependencies will be upgraded to v2.
* Implementation of logr which will produce logs in JSON format will be created.
* Flag which will inject new logr implementation into klog if --logging-format is set to json will be added to k8s.io/component-base JSON format
* Static analysis protects against adding new string formatting logs

#### GA

* All feedback for text and json logs output formats is addressed and both output formats become an API.

### Performance

Proposed changes could impact k8s performance in two ways:
* By introducing degrading in klog implementation
* By largely increasing log volume

#### Logger implementation performance

We have benchmarked proof of concept implementation of new logging format. Results measured on 30s benchmark for passing 2 arguments to format function.

|logger                 |time [ns/op]|bytes[B/op]|allocations[alloc/op]|
|-----------------------|------------|-----------|---------------------|
|Text Infof             |2252        |248        |3                    |
|Text InfoS             |2455        |280        |3                    |
|JSON Infof             |1406        |19         |1                    |
|JSON InfoS             |319         |67         |1                    |

InfoS implementation for text is 9% slower than Infof. This increase should not have a big impact on overall Kubernetes performance as logging takes less than 2% of overall CPU usage.
JSON implementation using zapr is much more efficient compared to current implementation.

#### Log volume increase analysis

Example log size change based on apiserver HTTP access log. This log generates most volume in kube-apiserver and accepts lots of arguments. This makes this message a good example of the worst case for log volume increase.

```go
klog.Infof("%s %s: (%v) %v%v%v [%s %s]", rl.req.Method, rl.req.RequestURI, latency, rl.status, rl.statusStack, rl.addedInfo, rl.req.UserAgent(), rl.req.RemoteAddr)
```

Using old format generates 206 characters (before [changes by @lavalamp](https://github.com/kubernetes/kubernetes/pull/87203))
```
I1025 00:15:15.525108       1 httplog.go:79] GET /api/v1/namespaces/kube-system/pods/metrics-server-v0.3.1-57c75779f-9p8wg: (1.512ms) 200 [pod_nanny/v0.0.0 (linux/amd64) kubernetes/$Format 10.56.1.19:51756]
```

Migrating to InfoS will result in log of length 284 characters [+38%]
```
I0129 14:57:28.832349  102767 experiment.go:26] HTTP method=GET uri=/api/v1/namespaces/kube-system/pods/metrics-server-v0.3.1-57c75779f-9p8wg latency=1.512ms status=200 status-stack= added-info= user-agent=”pod_nanny/v0.0.0 (linux/amd64) kubernetes/$Format” remote-addr=10.56.1.19:51756
```

InfoS with Json enabled will generate log line with 347 characters [+68%]
```json
{"level":"info","ts":1580306777.04728,"caller":"zapr@v0.1.1/zapr.go:69","msg":"HTTP","method":"GET","uri":"/api/v1/namespaces/kube-system/pods/metrics-server-v0.3.1-57c75779f-9p8wg","latency":"1.512ms","status":200,"status-stack":"","added-info":"","user-agent":"pod_nanny/v0.0.0 (linux/amd64) kubernetes/$Format","remote-addr":"10.56.1.19:51756"}
```

Data about byte log length in k8s components. Data taken from scalability test [gce-master-scale-performance](https://k8s-testgrid.appspot.com/sig-scalability-gce#gce-master-scale-performance)

|component              |average|50%ile|75%ile|90%ile|
|-----------------------|-------|------|------|------|
|kube-apiserver         |229    |218   |248   |319   |
|kube-controller-manager|255    |229   |355   |362   |
|kubelet                |759    |885   |912   |1143  |
|kube-scheduler         |217    |225   |226   |227   |

When looking at the whole repository we are not expecting a very big increase of log volume due added metadata. This is due to how big log messages are and low number of arguments passed to formatting messages (average below 2, max 10). Based on this HTTP access logs we are expecting that log volume will not increase by more than 50% for default (text) logging configuration.

### Future work

List of ideas that were put on the original proposal that we would like to tackle in future, when structured logging fundamentals are in place:
* Replacement of Klog for a better logging solutions (faster, lighter on dependencies)
* Log schema to ensure more uniform metadata
* Context-ful logging to support shared metadata with tracing

### Risks and Mitigations

#### Migration being abandoned halfway

Kubernetes is a huge project spread across multiple repositories with thousands of logging calls. As not all of those calls can be migrated to new structured methods at once.

To mitigate the impact of this risk:

* We will start migration from the most common log lines to maximize the benefits from this manual work.
* We will submit changes in separate commits to increase the chance of each of the changes being merged quickly.


#### Huge increase of log volume

This effort should result in reducing costs of log analysis in production setups. Introducing structured logs will allow for creating better indices and reducing the size of data needed for analysis. From a log throughput perspective there will be an increase of log size causing a pressure of log ingestion (disk, logging agents, logging API).

To reduce the potential impact we will try to minimize the number of key-values logged when performing manual migration and we will try to use short keys.

If log volume increase cannot be avoided we will include information about large increase in Kubernetes release change log so users can prepare additional capacity or change log verbosity.

## Design Details

|nr |line                        |Log volume|Coverage|
|---|----------------------------|----------|--------|
|2  |[get.go:251](https://github.com/kubernetes/kubernetes/blob/15c3f1b11/staging/src/k8s.io/apiserver/pkg/server/httplog/httplog.go#L90)     |12.48%    |96.92%  |
|3  |[watcher.go:363](https://github.com/kubernetes/kubernetes/blob/15c3f1b11/staging/src/k8s.io/apiserver/pkg/endpoints/handlers/get.go#L251)|0.83%     |97.76%  |
|4  |[watcher.go:256](https://github.com/kubernetes/kubernetes/blob/15c3f1b11/staging/src/k8s.io/apiserver/pkg/storage/etcd3/watcher.go#L363) |0.52%     |98.28%  |
|5  |[event.go:278](https://github.com/kubernetes/kubernetes/blob/15c3f1b11/staging/src/k8s.io/apiserver/pkg/storage/etcd3/watcher.go#L256)   |0.42%     |98.70%  |
|6  |[scheduler.go:715](https://github.com/kubernetes/kubernetes/blob/15c3f1b11/staging/src/k8s.io/client-go/tools/record/event.go#L278)      |0.33%     |99.03%  |
|7  |garbagecollector.go:404     |0.32%     |99.35%  |
|8  |garbagecollector.go:517     |0.32%     |99.67%  |
|9  |node_authorizer.go:197      |0.03%     |99.70%  |
|10 |controller_utils.go:603     |0.03%     |99.73%  |
|11 |replica_set.go:561          |0.03%     |99.76%  |
|12 |trace.go:116                |0.02%     |99.78%  |
|13 |deployment_controller.go:484|0.02%     |99.80%  |
|14 |deployment_controller.go:575|0.02%     |99.82%  |
|15 |endpoints_controller.go:340 |0.02%     |99.84%  |
|16 |cloud_cidr_allocator.go:233 |0.02%     |99.86%  |
|17 |ttl_controller.go:271       |0.01%     |99.87%  |
|18 |replica_set.go:597          |0.01%     |99.88%  |
|19 |replica_set.go:225          |0.01%     |99.88%  |
|20 |scheduler.go:604            |0.01%     |99.89%  |
|21 |factory.go:431              |0.01%     |99.90%  |
|22 |kubelet_getters.go:173      |0.01%     |99.90%  |


### Example of migrating klog call

Before migration:
```go
klog.Infof("%s %s: (%v) %v%v%v [%s %s]", rl.req.Method, rl.req.RequestURI, latency, rl.status, rl.statusStack, rl.addedInfo, rl.req.UserAgent(), rl.req.RemoteAddr)
```

After migration:
```go
klog.InfoS("Access", "method", rl.req.Method, "uri", rl.req.RequestURI, "latency", latency, "status", rl.status, "agent", rl.req.UserAgent(), "addr", rl.req.RemoteAddr)
```

## Alternatives

#### Just write guideline and update log messages

* Doesn’t guarantee uniformity of log messages, is error prone and vulnerable to drifting
* Doesn’t improve log ingestion into external systems
* Doesn’t provide any value when switching to JSON output format

#### Replace klog with some other structured logging library

* Requires replacing all calls to klog with calls to a new logging library which makes this change hard to implement and merge into source repository.
* Large percentage of the logs cannot be migrated to structured logging completely automatically, therefore full migration would require a significant amount of manual work and increase the risk of this change being abandoned.
* Makes rollback of the change very hard to implement.

#### Use glogr instead of proposed message structure

* Glogr uses JSON serialization for both keys and values
* Significantly increases log volume
* Lack of support of warning and fatal severity levels

## Code organisation / infrastructure needed

As a result of this effort we are expecting creation of:
* `k8s.io/component-base/metadata` - new module with utility functions for extracting metadata from k8s objects.

## Production Readiness Review Process

**Feature enablement and rollback**
* How can this feature be enabled / disabled in a live cluster? **Changing logging format in control plane components would require recreation of cluster**
* Can the feature be disabled once it has been enabled (i.e., can we roll back the enablement)? **Yes, reverting the change will only effect logs generated when feature was enabled. Rollback can be done by changing flag value to component**
* Will enabling / disabling the feature require downtime for the control plane? **Yes, changing flag passed will require restarting components**
* Will enabling / disabling the feature require downtime or reprovisioning of a node? **Yes, if we are changing logging configuration of kubelet will require node downtime**
* What happens if a cluster with this feature enabled is rolled back? What happens if it is subsequently upgraded again? **Temporary change in log format produced**
* Are there tests for this? **Klog implementation is covered by integration tests**

**Scalability**
* Will enabling / using the feature result in any new API calls? Describe them with their impact keeping in mind the supported limits (e.g. 5000 nodes per cluster, 100 pods/s churn): **No**
* Will enabling / using the feature result in supporting new API types? How many objects of that type will be supported (and how that translates to limitations for users)? **No**
* Will enabling / using the feature result in increasing size or count of the existing API objects? **No**
* Will enabling / using the feature result in increasing time taken by any operations covered by existing SLIs/SLOs (e.g. by adding additional work, introducing new steps in between, etc.)? **No**
* Will enabling / using the feature result in non-negligible increase of resource usage (CPU, RAM, disk IO, ...) in any components? Things to keep in mind include: additional in-memory state, additional non-trivial computations, excessive access to disks (including increased log volume), significant amount of data sent and/or received over network, etc. Think through this in both small and large cases, again with respect to the supported limits. **Yes, increased log volume increase below 50%**
* Rollout, Upgrade, and Rollback Planning
* Dependencies
    * Does this feature depend on any specific services running in the cluster (e.g., a metrics service)? **No**
    * How does this feature respond to complete failures of the services on which it depends? **n/a**
    * How does this feature respond to degraded performance or high error rates from services on which it depends? **n/a**
* Monitoring requirements
    * How can an operator determine if the feature is in use by workloads? **Need to verify specific component flag, or look into logs generated by it**
    * How can an operator determine if the feature is functioning properly? **By looking at the logs generated by component**
    * What are the service level indicators an operator can use to determine the health of the service? **n/a**
    * What are reasonable service level objectives for the feature? **Defining a proper SLO for logging should consider e2e delivery to backend. This would be outside of scope of kubernetes**
* Troubleshooting
    * What are the known failure modes? **Logging is a blocking API. This means that in case of full node disk, logging can halt the process**
    * How can those be detected via metrics or logs? **liveness probes**
    * What are the mitigations for each of those failure modes? **Monitoring free space on disk**
    * What are the most useful log messages and what logging levels do they require? **klog will not generate any logs by itself**
    * What steps should be taken if SLOs are not being met to determine the problem? **n/a**
